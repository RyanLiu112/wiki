<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Personal wiki"><meta name=author content="Runze Liu"><link href=https://ryanliu112.github.io/wiki/CS285/05.Policy%20Gradients/ rel=canonical><link href=../04.Introduction%20to%20Reinforcement%20Learning/ rel=prev><link href=../06.Actor-Critic%20Algorithms/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.4.3, mkdocs-material-9.1.15"><title>5. Policy Gradients - My-wiki</title><link rel=stylesheet href=../../assets/stylesheets/main.26e3688c.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ecc896b0.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../css/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#5-policy-gradients class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../.. title=My-wiki class="md-header__button md-logo" aria-label=My-wiki data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> My-wiki </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 5. Policy Gradients </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=查找> <a href=javascript:void(0) class="md-search__icon md-icon" title=分享 aria-label=分享 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/RyanLiu112/wiki title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> RyanLiu112/wiki </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Wiki </a> </li> <li class=md-tabs__item> <a href=../../CS/%E3%80%90C%2B%2B%E3%80%91STL/ class=md-tabs__link> CS </a> </li> <li class=md-tabs__item> <a href=../01.Introduction%20and%20Course%20Overview/ class="md-tabs__link md-tabs__link--active"> CS285 </a> </li> <li class=md-tabs__item> <a href=../../Math/Bayesian%20Optimization/ class=md-tabs__link> Math </a> </li> <li class=md-tabs__item> <a href=../../Offline%20RL/1.BCQ/ class=md-tabs__link> Offline RL </a> </li> <li class=md-tabs__item> <a href=../../Reinforcement%20Learning/1.Basis/ class=md-tabs__link> Reinforcement Learning </a> </li> <li class=md-tabs__item> <a href=../../Tools/Git/ class=md-tabs__link> Tools </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title=My-wiki class="md-nav__button md-logo" aria-label=My-wiki data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> My-wiki </label> <div class=md-nav__source> <a href=https://github.com/RyanLiu112/wiki title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> RyanLiu112/wiki </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> Wiki </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> CS <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> CS </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CS/%E3%80%90C%2B%2B%E3%80%91STL/ class=md-nav__link> 【C++】STL </a> </li> <li class=md-nav__item> <a href=../../CS/%E3%80%90C%2B%2B%E3%80%91%E6%8E%92%E5%BA%8F/ class=md-nav__link> 【C++】排序 </a> </li> <li class=md-nav__item> <a href=../../CS/%E3%80%90C%2B%2B%E3%80%91%E6%95%B0%E7%BB%84/ class=md-nav__link> 【C++】数组 </a> </li> <li class=md-nav__item> <a href=../../CS/%E3%80%90C%2B%2B%E3%80%91%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/ class=md-nav__link> 【C++】栈和队列 </a> </li> <li class=md-nav__item> <a href=../../CS/%E3%80%90C%2B%2B%E3%80%91%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/ class=md-nav__link> 【C++】输入输出 </a> </li> <li class=md-nav__item> <a href=../../CS/%E3%80%90C%2B%2B%E3%80%91%E9%93%BE%E8%A1%A8/ class=md-nav__link> 【C++】链表 </a> </li> <li class=md-nav__item> <a href=../../CS/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/ class=md-nav__link> 动态规划 </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_8> <label class=md-nav__link for=__nav_2_8 id=__nav_2_8_label tabindex=0> 算法基础课 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_8_label aria-expanded=false> <label class=md-nav__title for=__nav_2_8> <span class="md-nav__icon md-icon"></span> 算法基础课 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CS/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/ class=md-nav__link> 第一章 基础算法 </a> </li> <li class=md-nav__item> <a href=../../CS/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%90%9C%E7%B4%A2%E4%B8%8E%E5%9B%BE%E8%AE%BA/ class=md-nav__link> 第三章 搜索与图论 </a> </li> <li class=md-nav__item> <a href=../../CS/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ class=md-nav__link> 第二章 数据结构 </a> </li> <li class=md-nav__item> <a href=../../CS/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/ class=md-nav__link> 第五章 动态规划 </a> </li> <li class=md-nav__item> <a href=../../CS/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E8%B4%AA%E5%BF%83/ class=md-nav__link> 第六章 贪心 </a> </li> <li class=md-nav__item> <a href=../../CS/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E8%AF%BE/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/ class=md-nav__link> 第四章 数学知识 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> CS285 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> CS285 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../01.Introduction%20and%20Course%20Overview/ class=md-nav__link> 01.Introduction and Course Overview </a> </li> <li class=md-nav__item> <a href=../02.Supervised%20Learning%20of%20Behaviors/ class=md-nav__link> 02.Supervised Learning of Behaviors </a> </li> <li class=md-nav__item> <a href=../03.PyTorch%20Tutorial/ class=md-nav__link> 03.PyTorch Tutorial </a> </li> <li class=md-nav__item> <a href=../04.Introduction%20to%20Reinforcement%20Learning/ class=md-nav__link> 4. Introduction to Reinforcement Learning </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> 5. Policy Gradients <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> 5. Policy Gradients </a> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#51-policy-gradient class=md-nav__link> 5.1 Policy Gradient </a> <nav class=md-nav aria-label="5.1 Policy Gradient"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#evaluating-the-objective class=md-nav__link> Evaluating the objective </a> </li> <li class=md-nav__item> <a href=#directly-policy-differentiation class=md-nav__link> Directly policy differentiation </a> </li> <li class=md-nav__item> <a href=#reinforce class=md-nav__link> REINFORCE </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#52-understanding-policy-gradient class=md-nav__link> 5.2 Understanding Policy Gradient </a> <nav class=md-nav aria-label="5.2 Understanding Policy Gradient"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#partial-observability class=md-nav__link> Partial observability </a> </li> <li class=md-nav__item> <a href=#pg class=md-nav__link> PG 的缺点 </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#53-reducing-variance class=md-nav__link> 5.3 Reducing Variance </a> <nav class=md-nav aria-label="5.3 Reducing Variance"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#causality class=md-nav__link> Causality </a> </li> <li class=md-nav__item> <a href=#baselines class=md-nav__link> Baselines </a> </li> <li class=md-nav__item> <a href=#analyzing-variance class=md-nav__link> Analyzing variance </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#54-off-policy-policy-gradient class=md-nav__link> 5.4 Off-policy Policy Gradient </a> <nav class=md-nav aria-label="5.4 Off-policy Policy Gradient"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#policy-gradient-is-on-policy class=md-nav__link> Policy gradient is on-policy </a> </li> <li class=md-nav__item> <a href=#a-first-order-approximation-for-is-preview class=md-nav__link> A first-order approximation for IS (preview) </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#55-implementing-policy-gradients class=md-nav__link> 5.5 Implementing Policy Gradients </a> <nav class=md-nav aria-label="5.5 Implementing Policy Gradients"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#policy-gradient-with-automatic-differentiation class=md-nav__link> Policy gradient with automatic differentiation </a> </li> <li class=md-nav__item> <a href=#policy-gradient-in-practice class=md-nav__link> Policy gradient in practice </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#56-advanced-policy-gradients class=md-nav__link> 5.6 Advanced Policy Gradients </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../06.Actor-Critic%20Algorithms/ class=md-nav__link> 6. Actor-Critic Algorithms </a> </li> <li class=md-nav__item> <a href=../07.Value%20Function%20Methods/ class=md-nav__link> 7. Value Function Methods </a> </li> <li class=md-nav__item> <a href=../08.Deep%20RL%20with%20Q-Functions/ class=md-nav__link> 08.Deep RL with Q Functions </a> </li> <li class=md-nav__item> <a href=../09.Advanced%20Policy%20Gradients/ class=md-nav__link> 09.Advanced Policy Gradients </a> </li> <li class=md-nav__item> <a href=../10.Optimal%20Control%20and%20Planning/ class=md-nav__link> 10.Optimal Control and Planning </a> </li> <li class=md-nav__item> <a href=../11.Model-Based%20Reinforcement%20Learning/ class=md-nav__link> 11.Model Based Reinforcement Learning </a> </li> <li class=md-nav__item> <a href=../12.Model-Based%20Policy%20Learning/ class=md-nav__link> 12.Model Based Policy Learning </a> </li> <li class=md-nav__item> <a href=../13.Exploration%20%28Part%201%29/ class=md-nav__link> 13.Exploration (Part 1) </a> </li> <li class=md-nav__item> <a href=../14.Exploration%20%28Part%202%29/ class=md-nav__link> 14.Exploration (Part 2) </a> </li> <li class=md-nav__item> <a href=../15.Offline%20Reinforcement%20Learning%20%28Part%201%29/ class=md-nav__link> 15.Offline Reinforcement Learning (Part 1) </a> </li> <li class=md-nav__item> <a href=../16.Offline%20Reinforcement%20Learning%20%28Part%202%29/ class=md-nav__link> 16.Offline Reinforcement Learning (Part 2) </a> </li> <li class=md-nav__item> <a href=../17.Reinforcement%20Learning%20Theory%20Basics/ class=md-nav__link> 17.Reinforcement Learning Theory Basics </a> </li> <li class=md-nav__item> <a href=../18.Variational%20Inference%20and%20Generative%20Models/ class=md-nav__link> 18.Variational Inference and Generative Models </a> </li> <li class=md-nav__item> <a href=../19.Connection%20between%20Inference%20and%20Control/ class=md-nav__link> 19.Connection between Inference and Control </a> </li> <li class=md-nav__item> <a href=../20.Inverse%20Reinforcement%20Learning/ class=md-nav__link> 20.Inverse Reinforcement Learning </a> </li> <li class=md-nav__item> <a href=../22.Meta-Learning%20and%20Transfer%20Learning/ class=md-nav__link> 22.Meta Learning and Transfer Learning </a> </li> <li class=md-nav__item> <a href=../23.Challenges%20and%20Open%20Problems/ class=md-nav__link> 23.Challenges and Open Problems </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> Math <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Math </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Math/Bayesian%20Optimization/ class=md-nav__link> Bayesian Optimization </a> </li> <li class=md-nav__item> <a href=../../Math/Optimal%20Transport/ class=md-nav__link> Optimal Transport </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> Offline RL <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Offline RL </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Offline%20RL/1.BCQ/ class=md-nav__link> Off-Policy Deep Reinforcement Learning without Exploration (ICML'19) </a> </li> <li class=md-nav__item> <a href=../../Offline%20RL/2.BEAR/ class=md-nav__link> RL </a> </li> <li class=md-nav__item> <a href=../../Offline%20RL/3.Diffuser/ class=md-nav__link> RL </a> </li> <li class=md-nav__item> <a href=../../Offline%20RL/4.Decision%20Transformer/ class=md-nav__link> RL </a> </li> <li class=md-nav__item> <a href=../../Offline%20RL/5.Trajectory%20Transformer/ class=md-nav__link> RL </a> </li> <li class=md-nav__item> <a href=../../Offline%20RL/6.Dicision%20Diffuser/ class=md-nav__link> RL </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> Reinforcement Learning <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Reinforcement Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Reinforcement%20Learning/1.Basis/ class=md-nav__link> 强化学习基础 </a> </li> <li class=md-nav__item> <a href=../../Reinforcement%20Learning/2.Markov%20Decision%20Process/ class=md-nav__link> 马尔可夫决策过程 </a> </li> <li class=md-nav__item> <a href=../../Reinforcement%20Learning/3.Dynamic%20Programming/ class=md-nav__link> 动态规划 </a> </li> <li class=md-nav__item> <a href=../../Reinforcement%20Learning/4.Monte%20Carlo%20Methods/ class=md-nav__link> 蒙特卡罗 </a> </li> <li class=md-nav__item> <a href=../../Reinforcement%20Learning/5.Temporal-Difference%20Learning/ class=md-nav__link> 时序差分学习 </a> </li> <li class=md-nav__item> <a href=../../Reinforcement%20Learning/6.Deep%20Q-Learning/ class=md-nav__link> 深度 Q 网络 </a> </li> <li class=md-nav__item> <a href=../../Reinforcement%20Learning/7.Policy%20Gradient/ class=md-nav__link> 策略梯度 </a> </li> <li class=md-nav__item> <a href=../../Reinforcement%20Learning/8.Actor-Critic/ class=md-nav__link> RL </a> </li> <li class=md-nav__item> <a href=../../Reinforcement%20Learning/9.Imitation%20Learning/ class=md-nav__link> RL </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> Tools <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Tools </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Tools/Git/ class=md-nav__link> Git: fork 后的仓库如何与原仓库同步 </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href="https://github.com/RyanLiu112/wiki/edit/master/docs/CS285/05.Policy Gradients.md" title=编辑此页 class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg> </a> <a href="https://github.com/RyanLiu112/wiki/raw/master/docs/CS285/05.Policy Gradients.md" title=查看本页的源代码 class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg> </a> <h1 id=5-policy-gradients>5. Policy Gradients<a class=headerlink href=#5-policy-gradients title="Permanent link">&para;</a></h1> <h2 id=51-policy-gradient>5.1 Policy Gradient<a class=headerlink href=#51-policy-gradient title="Permanent link">&para;</a></h2> <div class=arithmatex>\[ p_\theta(\tau) = p_\theta(\mathbf{s}_1, \mathbf{a}_1, \ldots, \mathbf{s}_T, \mathbf{a}_T)=p(\mathbf{s}_1) \prod_{t=1}^T \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t) p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t) \]</div> <p>其中 <span class=arithmatex>\(p(\mathbf{s}_1)\)</span> 和 <span class=arithmatex>\(p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t)\)</span> 都是未知的，我们仅仅假设 agent 可以与环境交互</p> <h3 id=evaluating-the-objective>Evaluating the objective<a class=headerlink href=#evaluating-the-objective title="Permanent link">&para;</a></h3> <div class=arithmatex>\[ \theta^{\star}=\arg \max _\theta \underbrace{E_{\tau \sim p_\theta(\tau)}\left[\sum_t r(\mathbf{s}_t, \mathbf{a}_t)\right]}_{J(\theta)} \]</div> <p>可以使用 <span class=arithmatex>\(\pi_\theta\)</span> 采样 <span class=arithmatex>\(N\)</span> 条轨迹，得到 <span class=arithmatex>\(J(\theta)\)</span> 的无偏估计：</p> <div class=arithmatex>\[ J(\theta)=E_{\tau \sim p_\theta(\tau)}\left[\sum_t r(\mathbf{s}_t, \mathbf{a}_t)\right] \approx \frac{1}{N} \sum_i \sum_t r(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}) \]</div> <h3 id=directly-policy-differentiation>Directly policy differentiation<a class=headerlink href=#directly-policy-differentiation title="Permanent link">&para;</a></h3> <div class=arithmatex>\[ \begin{gathered} J(\theta)=E_{\tau \sim p_\theta(\tau)}[\underbrace{r(\tau)}]=\int p_\theta(\tau) r(\tau) d \tau \\ \sum_{t=1}^T r(\mathbf{s}_t, \mathbf{a}_t) \end{gathered} \]</div> <p>由于 Log-gradient trick</p> <div class=arithmatex>\[ \nabla_\theta p_\theta(\tau) = p_\theta(\tau) \frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)} = p_\theta(\tau) \nabla_\theta \log p_\theta(\tau) \]</div> <p>故</p> <div class=arithmatex>\[ \nabla_\theta J(\theta)=\int \nabla_\theta p_\theta(\tau) r(\tau) d \tau=\int p_\theta(\tau) \nabla_\theta \log p_\theta(\tau) r(\tau) d \tau=E_{\tau \sim p_\theta(\tau)}\left[\nabla_\theta \log p_\theta(\tau) r(\tau)\right] \]</div> <p>由于</p> <div class=arithmatex>\[ p_\theta(\tau) = p_\theta(\mathbf{s}_1, \mathbf{a}_1, \ldots, \mathbf{s}_T, \mathbf{a}_T)=p(\mathbf{s}_1) \prod_{t=1}^T \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t) p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t) \]</div> <p>两边同时取 log</p> <div class=arithmatex>\[ \log p_\theta(\tau)=\log p\left(\mathbf{s}_1\right)+\sum_{t=1}^T \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t\right)+\log p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t\right) \]</div> <p>两边同时对 <span class=arithmatex>\(\theta\)</span> 求梯度</p> <div class=arithmatex>\[ \begin{aligned} \nabla_\theta \log p_\theta(\tau) =&amp; \nabla_\theta \left[\log p(\mathbf{s}_1) + \sum_{t=1}^T \log \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t) + \log p(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t) \right] \\ =&amp; \nabla_\theta \sum_{t=1}^T \log \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t) \\ =&amp; \sum_{t=1}^T \nabla_\theta \log \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t) \\ \end{aligned} \]</div> <p>因此</p> <div class=arithmatex>\[ \begin{aligned} \nabla_\theta J(\theta) =&amp; E_{\tau \sim p_\theta(\tau)} \left[\nabla_\theta \log p_\theta(\tau) r(\tau)\right] \\ =&amp; E_{\tau \sim p_\theta(\tau)} \left[\left(\sum_{t=1}^T \nabla_\theta \log \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t)\right)\left(\sum_{t=1}^T r(\mathbf{s}_t, \mathbf{a}_t)\right)\right] \end{aligned} \]</div> <p>采样</p> <div class=arithmatex>\[ \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N\left(\sum_{t=1}^T \nabla_\theta \log \pi_\theta(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})\right)\left(\sum_{t=1}^T r(\mathbf{s}_{i, t}, \mathbf{a}_{i, t})\right) \]</div> <h3 id=reinforce>REINFORCE<a class=headerlink href=#reinforce title="Permanent link">&para;</a></h3> <pre class=mermaid><code>graph LR
    A["1. 使用当前策略采集一个 batch 的轨迹"] --&gt; B["2. 计算 J(θ) 相对于 θ 的梯度"];
    B --&gt; C["3. 梯度上升更新 θ"];
    C --&gt; A;</code></pre> <h2 id=52-understanding-policy-gradient>5.2 Understanding Policy Gradient<a class=headerlink href=#52-understanding-policy-gradient title="Permanent link">&para;</a></h2> <p>BC 只是单纯最大化专家轨迹的 likelihood，</p> <p>PG 可以看作对 maximum likelihood 加权，增加好轨迹的 likelihood，减少坏轨迹的 likelihood</p> <h3 id=partial-observability>Partial observability<a class=headerlink href=#partial-observability title="Permanent link">&para;</a></h3> <p>states 满足马尔可夫性质（给定当前状态，未来状态与过去状态条件独立），但是 observations 通常不满足。</p> <p>由于 PG 不需要马尔可夫性质，所以部分可观的 PG 与全可观 PG 的推导相同，只需要把 states 换成 observations，PG 可以直接用于 POMDP。</p> <h3 id=pg>PG 的缺点<a class=headerlink href=#pg title="Permanent link">&para;</a></h3> <ul> <li>方差大：PG 的方差取决于遇到的样本</li> </ul> <h2 id=53-reducing-variance>5.3 Reducing Variance<a class=headerlink href=#53-reducing-variance title="Permanent link">&para;</a></h2> <h3 id=causality>Causality<a class=headerlink href=#causality title="Permanent link">&para;</a></h3> <p><em>Causality</em>: 当 <span class=arithmatex>\(t &lt; t'\)</span> 时，时刻 <span class=arithmatex>\(t'\)</span> 的策略无法影响时刻 <span class=arithmatex>\(t\)</span> 的奖励。</p> <p>不同于马尔可夫性，Causality 在任何情况下都是成立的，但马尔可夫性未必。</p> <p>前面的 PG 并未用到这个假设，实际上，可以利用这个假设减小方差。</p> <p><span class=arithmatex>\(\nabla J(\theta)\)</span> 中有 log pi 和对应轨迹 return 的乘积，根据 Causality，<strong>当前</strong>时刻的策略无法影响<strong>过去</strong>时刻的奖励，因此每一项 log pi 只需要乘以<strong>现在</strong>和<strong>未来</strong>时刻的奖励，且改变后的 estimator 仍然是无偏的。</p> <p>由于移除了部分项，求和后的值比原来更小了，求期望之后值也相应减小，从而减小了方差。</p> <div class=arithmatex>\[ \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}) \left(\sum_{t'=t}^T r(\mathbf{s}_{i, t'}, \mathbf{a}_{i, t'})\right) \]</div> <p>其中从当前时刻 <span class=arithmatex>\(t\)</span> 开始的 reward 之和也被称为 <code>reward to go</code>，记作 <span class=arithmatex>\(\hat{Q}_{i,t} = \sum_{t'=t}^T r(\mathbf{s}_{i, t'}, \mathbf{a}_{i, t'})\)</span>。</p> <h3 id=baselines>Baselines<a class=headerlink href=#baselines title="Permanent link">&para;</a></h3> <div class=arithmatex>\[ \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \nabla_\theta \log p_\theta(\tau)[r(\tau)-b] \]</div> <p>其中 <span class=arithmatex>\(<span class=arithmatex>\(b=\frac{1}{N} \sum_{i=1}^N r(\tau)\)</span>\)</span>。</p> <p>由于</p> <div class=arithmatex>\[ E\left[\nabla_\theta \log p_\theta(\tau) b\right]=\int p_\theta(\tau) \nabla_\theta \log p_\theta(\tau) b d \tau=\int \nabla_\theta p_\theta(\tau) b d \tau=b \nabla_\theta \int p_\theta(\tau) d \tau=b \nabla_\theta 1=0 \]</div> <p>因此减去 baseline 仍然是无偏的，但由于</p> <div class=arithmatex>\[ \begin{aligned} Var(X-\bar{X}) &amp;= E[(X-\bar{X}-E[X-\bar{X}])^2] \\ &amp;= E[X^2-2X\bar{X}+\bar{X}^2] \\ &amp;= EX^2-2E^2X+E\bar{X}^2 \\ &amp;= EX^2-2E^2X+E^2\bar{X}+Var(\bar{X}) \\ &amp;= EX^2-E^2X+Var(\bar{X}) \\ &amp;= EX^2-E^2X+Var(X)/n \\ &amp;&lt; EX^2-E^2X \\ &amp;= Var(X) \end{aligned} \]</div> <p>方差比原来有所减小。然而，我们无法找到最优的 baseline。</p> <h3 id=analyzing-variance>Analyzing variance<a class=headerlink href=#analyzing-variance title="Permanent link">&para;</a></h3> <div class=arithmatex>\[ \operatorname{Var}[X]=E[X^2]-E[X]^2 \]</div> <p>方差为：</p> <div class=arithmatex>\[ \operatorname{Var}=E_{\tau \sim p_\theta(\tau)}\left[\left(\nabla_\theta \log p_\theta(\tau)(r(\tau)-b)\right)^2\right]-E_{\tau \sim p_\theta(\tau)}\left[\nabla_\theta \log p_\theta(\tau)(r(\tau)-b)\right]^2 \]</div> <p>将方差对 baseline 求导：</p> <div class=arithmatex>\[ \begin{aligned} \frac{d \operatorname{Var}}{d b} &amp;= \frac{d}{d b} E\left[g(\tau)^2(r(\tau)-b)^2\right] \\ &amp;= \frac{d}{d b}\left(E\left[q(\tau)^2 r(\tau)^2\right]-2 E\left[g(\tau)^2 r(\tau) b\right]+b^2 E\left[g(\tau)^2\right]\right) \\ &amp;= -2 E\left[g(\tau)^2 r(\tau)\right]+2 b E\left[g(\tau)^2\right] \\ &amp;=0 \end{aligned} \]</div> <p>因此能达到最小方差的最优 baseline 为：</p> <div class=arithmatex>\[ b=\frac{E\left[g(\tau)^2 r(\tau)\right]}{E\left[g(\tau)^2\right]} \]</div> <p>然而在实际中，我们通常不使用最优 baseline，而使用 return 均值作为 baseline。</p> <h2 id=54-off-policy-policy-gradient>5.4 Off-policy Policy Gradient<a class=headerlink href=#54-off-policy-policy-gradient title="Permanent link">&para;</a></h2> <h3 id=policy-gradient-is-on-policy>Policy gradient is on-policy<a class=headerlink href=#policy-gradient-is-on-policy title="Permanent link">&para;</a></h3> <p>由于 REINFORCE 算法需要先用当前策略采数据，再利用这些数据做更新，因此 PG 是 <strong>on-policy</strong> 的。</p> <p>On-policy 学习十分低效（inefficient）</p> <p>由于每次更新时，策略参数只有很小的改变，</p> <p>Importance sampling（重要性采样）</p> <div class=arithmatex>\[ \begin{aligned} E_{x \sim p(x)}[f(x)] &amp;= \int p(x) f(x) d x \\ &amp;= \int \frac{q(x)}{q(x)} p(x) f(x) d x \\ &amp;= \int q(x) \frac{p(x)}{q(x)} f(x) d x \\ &amp;= E_{x \sim q(x)}\left[\frac{p(x)}{q(x)} f(x)\right] \end{aligned} \]</div> <p>我们希望将算法调整为 off-policy，假如我们现在有来自另一分布 <span class=arithmatex>\(\bar{p}(\tau)\)</span> 的轨迹，</p> <div class=arithmatex>\[ \nabla_{\theta^{\prime}} J(\theta^{\prime}) = E_{\tau \sim p_\theta(\tau)}\left[\frac{\nabla_{\theta^{\prime}} p_{\theta^{\prime}}(\tau)}{p_\theta(\tau)} r(\tau)\right] = E_{\tau \sim p_\theta(\tau)}\left[\frac{p_{\theta^{\prime}}(\tau)}{p_\theta(\tau)} \nabla_{\theta^{\prime}} \log p_{\theta^{\prime}}(\tau) r(\tau)\right] \]</div> <p>因此</p> <div class=arithmatex>\[ \begin{aligned} \nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right) &amp;= E_{\tau \sim p_\theta(\tau)}\left[\frac{p_{\theta^{\prime}}(\tau)}{p_\theta(\tau)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\tau) r(\tau)\right] \quad \text { when } \theta \neq \theta^{\prime} \\ &amp;= E_{\tau \sim p_\theta(\tau)}\left[\left(\prod_{t=1}^T \frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_t \mid \mathbf{s}_t\right)}{\pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t\right)}\right)\left(\sum_{t=1}^T \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_t \mid \mathbf{s}_t\right)\right)\left(\sum_{t=1}^T r\left(\mathbf{s}_t, \mathbf{a}_t\right)\right)\right] \\ &amp;= E_{\tau \sim p_\theta(\tau)}\left[\sum_{t=1}^T \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\mathbf{a}_t \mid \mathbf{s}_t) \left(\prod_{t'=1}^t \frac{\pi_{\theta^{\prime}}(\mathbf{a}_{t'} \mid \mathbf{s}_{t'})}{\pi_\theta(\mathbf{a}_{t'} \mid \mathbf{s}_{t'})}\right) \left(\sum_{t'=t}^T r(\mathbf{s}_{t'}, \mathbf{a}_{t'}) \left(\prod_{t''=t}^{t'} \frac{\pi_{\theta^{\prime}}(\mathbf{a}_{t''} \mid \mathbf{s}_{t''})}{\pi_\theta(\mathbf{a}_{t''} \mid \mathbf{s}_{t''})}\right) \right)\right] \\ \end{aligned} \]</div> <h3 id=a-first-order-approximation-for-is-preview>A first-order approximation for IS (preview)<a class=headerlink href=#a-first-order-approximation-for-is-preview title="Permanent link">&para;</a></h3> <div class=arithmatex>\[ \begin{aligned} \nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right) &amp;\approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \frac{\pi_{\theta^{\prime}}\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)}{\pi_\theta\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right) \hat{Q}_{i, t} \\ &amp;= \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \frac{\pi_{\theta^{\prime}}(\mathbf{s}_{i, t})}{\pi_\theta(\mathbf{s}_{i, t})} \frac{\pi_{\theta^{\prime}}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})}{\pi_\theta(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}) \hat{Q}_{i, t} \\ &amp;\approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \frac{\pi_{\theta^{\prime}}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})}{\pi_\theta(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t})} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}) \hat{Q}_{i, t} \end{aligned} \]</div> <p>其中最后一步，当 <span class=arithmatex>\(\theta'\)</span> 与 <span class=arithmatex>\(\theta\)</span> 较为接近时，可以进行近似。</p> <h2 id=55-implementing-policy-gradients>5.5 Implementing Policy Gradients<a class=headerlink href=#55-implementing-policy-gradients title="Permanent link">&para;</a></h2> <h3 id=policy-gradient-with-automatic-differentiation>Policy gradient with automatic differentiation<a class=headerlink href=#policy-gradient-with-automatic-differentiation title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=c1># Given:</span>
<a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=c1># actions - (N*T) x Da tensor of actions</span>
<a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=c1># states - (N*T) x Ds tensor of states</span>
<a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=c1># q_values – (N*T) x 1 tensor of estimated state-action values</span>
<a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=c1># Build the graph:</span>
<a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=n>logits</span> <span class=o>=</span> <span class=n>policy</span><span class=o>.</span><span class=n>predictions</span><span class=p>(</span><span class=n>states</span><span class=p>)</span> <span class=c1># This should return (N*T) x Da tensor of action logits</span>
<a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=n>negative_likelihoods</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>softmax_cross_entropy_with_logits</span><span class=p>(</span><span class=n>labels</span><span class=o>=</span><span class=n>actions</span><span class=p>,</span> <span class=n>logits</span><span class=o>=</span><span class=n>logits</span><span class=p>)</span>
<a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a><span class=n>weighted_negative_likelihoods</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=n>negative_likelihoods</span><span class=p>,</span> <span class=n>q_values</span><span class=p>)</span>
<a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a><span class=n>loss</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>weighted_negative_likelihoods</span><span class=p>)</span>
<a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a><span class=n>gradients</span> <span class=o>=</span> <span class=n>loss</span><span class=o>.</span><span class=n>gradients</span><span class=p>(</span><span class=n>loss</span><span class=p>,</span> <span class=n>variables</span><span class=p>)</span>
</code></pre></div> <h3 id=policy-gradient-in-practice>Policy gradient in practice<a class=headerlink href=#policy-gradient-in-practice title="Permanent link">&para;</a></h3> <p>为了使用自动微分工具，在实现时，我们将 PG 的 pseudo loss 写成加权版的 maximum likelihood</p> <ul> <li>Gradient 具有高方差！</li> <li>因此需要考虑使用大 batch size（几千、几万）</li> <li>调整学习率非常困难（ADAM 还可以，但 SGD (with Momentum) 比较难用）</li> </ul> <h2 id=56-advanced-policy-gradients>5.6 Advanced Policy Gradients <a href=https://youtu.be/PEzuojy8lVo><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M23.498 6.186a3.016 3.016 0 0 0-2.122-2.136C19.505 3.545 12 3.545 12 3.545s-7.505 0-9.377.505A3.017 3.017 0 0 0 .502 6.186C0 8.07 0 12 0 12s0 3.93.502 5.814a3.016 3.016 0 0 0 2.122 2.136c1.871.505 9.376.505 9.376.505s7.505 0 9.377-.505a3.015 3.015 0 0 0 2.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/></svg></span></a><a class=headerlink href=#56-advanced-policy-gradients title="Permanent link">&para;</a></h2> </article> </div> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=页脚> <a href=../04.Introduction%20to%20Reinforcement%20Learning/ class="md-footer__link md-footer__link--prev" aria-label="上一页: 4. Introduction to Reinforcement Learning" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> 上一页 </span> <div class=md-ellipsis> 4. Introduction to Reinforcement Learning </div> </div> </a> <a href=../06.Actor-Critic%20Algorithms/ class="md-footer__link md-footer__link--next" aria-label="下一页: 6. Actor-Critic Algorithms" rel=next> <div class=md-footer__title> <span class=md-footer__direction> 下一页 </span> <div class=md-ellipsis> 6. Actor-Critic Algorithms </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2023 Runze Liu </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.sections", "navigation.tabs", "navigation.top", "search.highlight", "search.share", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../../assets/javascripts/bundle.b4d07000.min.js></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=../../javascripts/config.js></script> </body> </html>